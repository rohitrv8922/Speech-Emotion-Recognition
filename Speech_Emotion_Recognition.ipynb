{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZrr50+rwZ6eB/FPphDShq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitrv8922/Speech-Emotion-Recognition/blob/main/Speech_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPEECH EMOTION RECOGNITION\n",
        "**TABLE OF CONTENTS**\n",
        "\n",
        "1.INTRODUCTION\n",
        "\n",
        "2.EXPLORATORY DATA ANALYSIS (EDA)\n",
        "\n",
        "3.DATA AUGMENTATION\n",
        "\n",
        "4.FEATURE EXTRACTION\n",
        "\n",
        "5.MODEL"
      ],
      "metadata": {
        "id": "aaggoNJvfL0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INTRODUCTION\n",
        "\n",
        "Verbal Communication is valuable and sought after in workplace and classroom environments alike. There is no denying the notion that Indians lack verbal communication and consequently lag behind in the workplace or classroom environments. This happens despite them having strong technical competencies. Clear and comprehensive speech is the vital backbone of strong communication and presentation skills. Where some occupations consist mainly of presenting, most careers require and thrive from the ability to communicate effectively. Research has shown that verbal communication remains one of the most employable skills in both the perception of employers and new graduates. Of the possible improvements to vocal presentations tone, disfluencies, and stutters, in particular, remain one of the most common and prominent factors of someoneâ€™s demonstration. Millions of people are affected by stuttering and other speech disfluencies, with the majority of the world having experienced mild stutters while communicating under stressful conditions. Research shows that mild disfluencies can be cured without medical help, just practicing speech regularly and constructive feedbacks are effective ways to improve. We, Data Scientists recognize this problem and say hello"
      ],
      "metadata": {
        "id": "KPTWTUklfL_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROBLEM STATEMENT**"
      ],
      "metadata": {
        "id": "gqiZdG3poLvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose is to recognize the emotion and affective state of the speaker from his/her speech signal."
      ],
      "metadata": {
        "id": "zK4J698goP7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA SOURCE USED**"
      ],
      "metadata": {
        "id": "5D_-I_fDfMcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. Here's the filename identifiers as per the official RAVDESS website:\n",
        "\n",
        "1.Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)\n",
        "This dataset includes around 1500 audio file input from 24 different actors. 12 male and 12 female where these actors record short audios in 8 different emotions i.e 1 = neutral, 2 = calm, 3 = happy, 4 = sad, 5 = angry, 6 = fearful, 7 = disgust, 8 = surprised. Each audio file is named in such a way that the 7th character is consistent with the different emotions that they represent.\n",
        "\n",
        "2.Surrey Audio-Visual Expressed Emotion (Savee)\n",
        "This dataset contains around 500 audio files recorded by 4 different male actors. The first two characters of the file name correspond to the different emotions that the potray."
      ],
      "metadata": {
        "id": "gu_g6oDhqHy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLORATORY DATA ANALYSIS"
      ],
      "metadata": {
        "id": "aQOLi2qwr1C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Audio\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import itertools"
      ],
      "metadata": {
        "id": "E9QIp-PafNHq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4v75SR71RsK",
        "outputId": "18c72334-bd33-4ac8-8fa5-bdfb8017e66b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess = '/content/drive/MyDrive/Ravdess/'\n",
        "Savee = '/content/drive/MyDrive/Savee/'"
      ],
      "metadata": {
        "id": "xZjxbDDfGQLG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "\n",
        "emotion_df = []\n",
        "\n",
        "for dir in ravdess_directory_list:\n",
        "    actor = os.listdir(os.path.join(Ravdess, dir))\n",
        "    for wav in actor:\n",
        "        info = wav.partition(\".wav\")[0].split(\"-\")\n",
        "        emotion = int(info[2])\n",
        "        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))\n",
        "\n",
        "Ravdess_df = pd.DataFrame.from_dict(emotion_df)\n",
        "Ravdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n",
        "\n",
        "Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "Ravdess_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "z7YHNMHwFcDz",
        "outputId": "ab0209ec-518d-4a65-a671-4bd7a01281cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0ac339598225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0memotion_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRavdess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths for data.\n",
        "Ravdess = \"/content/gdrive/MyDrive/Ravdess/acotrs_speech_audios_01-24/\"\n",
        "Savee = \"/content/gdrive/MyDrive/Deep_learning_Project/surrey-audiovisual-expressed-emotion-savee/\""
      ],
      "metadata": {
        "id": "H74G0Lkr7VsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/drive/folders/1Nod70b0svNs9ho6fK83KXpaTM4QjPbI3?usp=sharing"
      ],
      "metadata": {
        "id": "jiosEZkiCvQm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}